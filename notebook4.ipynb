{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet M2 MIAS : Benchmark Avancé des Risques de Réidentification\n",
    "\n",
    "**Auteur :** [Votre Nom]\n",
    "**Superviseur :** Pr Emmanuel Chazard\n",
    "\n",
    "## 1. Objectif\n",
    [cite_start]"Ce notebook propose un benchmark exhaustif de **15 techniques** pour évaluer les risques d'anonymisation, réparties sur les 3 critères du G29 [cite: 30-32].\n",
    "\n",
    "### Les 15 Techniques Implémentées :\n",
    "\n",
    "#### A. Individualisation (Capacité à isoler)\n",
    "1.  **Attaque Procureur ($k=1$)** : Recherche de combinaisons uniques exactes (Méthode classique).\n",
    "2.  **Distance de Hamming (Record Linkage)** : Recherche du profil le plus proche (tolérance aux erreurs/mélanges).\n",
    "3.  **Attaque par Machine Learning (Random Forest)** : Un modèle prédictif tente de deviner l'ID à partir des attributs.\n",
    "4.  **Score d'Entropie (Information)** : Quantité d'information unique fournie par les quasi-identifiants.\n",
    "5.  **Risque Moyen (Average Risk)** : Probabilité moyenne de réidentification correcte sur l'ensemble de la population (1/taille du groupe).\n",
    "\n",
    "#### B. Inférence (Capacité à déduire)\n",
    "1.  **Vérité Terrain (Succès Exact)** : Comparaison directe avec la valeur réelle (Méthode \"Dieu\").\n",
    "2.  **l-diversité Théorique (1/l)** : Probabilité de deviner au hasard dans un groupe d'équivalence.\n",
    "3.  **K-Nearest Neighbors (KNN)** : Prédiction de la valeur sensible basée sur les $k$ voisins les plus proches.\n",
    "4.  **Attaque par Classification (ML)** : Un classifieur entrainé cherche à prédire la maladie.\n",
    "5.  **Inférence Bayésienne** : Gain d'information a posteriori sur la distribution de la variable sensible.\n",
    "\n",
    "#### C. Corrélation (Conservation de structure)\n",
    "1.  **Corrélation de Spearman** : Comparaison des rangs (ordre).\n",
    "2.  **V de Cramer** : Association nominale (adapté aux catégories).\n",
    "3.  **Information Mutuelle (MI)** : Dépendance non-linéaire et entropique.\n",
    "4.  **U de Theil** : Mesure d'association asymétrique (incertitude).\n",
    "5.  **Similarité PCA (Eigenvalues)** : Comparaison de la variance expliquée (structure globale des données).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import chi2_contingency, entropy, spearmanr\n",
    "from sklearn.metrics import mutual_info_score, accuracy_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration graphique\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Données\n",
    "Nous chargeons la base de référence et les fichiers variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"projets_donnees\"\n",
    "\n",
    "def load_data(directory):\n",
    "    data = {}\n",
    "    ref_path = os.path.join(directory, \"connaissances_externes.txt\")\n",
    "    if os.path.exists(ref_path):\n",
    "        data['reference'] = pd.read_csv(ref_path, sep=None, engine='python')\n",
    "        print(f\"Base référence chargée : {len(data['reference'])} lignes\")\n",
    "    \n",
    "    files = glob.glob(os.path.join(directory, \"out_*.txt\"))\n",
    "    for filepath in files:\n",
    "        name_key = os.path.basename(filepath).replace('.txt', '')\n",
    "        try:\n",
    "            data[name_key] = pd.read_csv(filepath, sep=None, engine='python')\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur {name_key} : {e}\")\n",
    "    return data\n",
    "\n",
    "datasets = load_data(DATA_DIR)\n",
    "df_reference = datasets['reference']\n",
    "\n",
    "# Vérité Terrain (Fichier sans mélange)\n",
    "if 'out_direct_0' in datasets:\n",
    "    df_truth = datasets['out_direct_0'].set_index('id_sejour')\n",
    "elif 'out_sample_0' in datasets:\n",
    "    df_truth = datasets['out_sample_0'].set_index('id_sejour')\n",
    "else:\n",
    "    df_truth = None\n",
    "    print(\"ATTENTION : Pas de vérité terrain (out_direct_0) trouvée. Certaines métriques d'inférence seront nulles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implémentation des 15 Techniques (Correction AttributeError)\n",
    "\n",
    "Cette classe encapsule toutes les méthodes de calcul. La méthode `ind_5_average_risk` a été corrigée pour utiliser `transform('size')`, ce qui évite l'erreur `AttributeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskBenchmark:\n",
    "    def __init__(self, df_ref, df_anon, qi_cols, sensitive_col, df_truth):\n",
    "        self.df_ref = df_ref\n",
    "        self.df_anon = df_anon\n",
    "        self.qi = [c for c in qi_cols if c in df_anon.columns]\n",
    "        self.sens = sensitive_col\n",
    "        self.truth = df_truth\n",
    "        \n",
    "        # Encodage numérique pour les algos ML (KNN, RF, PCA)\n",
    "        self.encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "        if self.qi:\n",
    "            self.X_ref = self.encoder.fit_transform(self.df_ref[self.qi].astype(str))\n",
    "            self.X_anon = self.encoder.transform(self.df_anon[self.qi].astype(str))\n",
    "        else:\n",
    "            self.X_ref, self.X_anon = None, None\n",
    "\n",
    "    # ================= INDIVIDUALISATION =================\n",
    "    \n",
    "    def ind_1_prosecutor(self):\n",
    "        \"\"\"1. Attaque Procureur (Unicité exacte)\"\"\"\n",
    "        if not self.qi: return 0.0\n",
    "        counts = self.df_anon.groupby(self.qi).size()\n",
    "        uniques = counts[counts == 1].index\n",
    "        if len(uniques) == 0: return 0.0\n",
    "        \n",
    "        df_unique = self.df_anon.set_index(self.qi)\n",
    "        df_unique = df_unique[df_unique.index.isin(uniques)].reset_index()\n",
    "        merged = pd.merge(df_unique, self.df_ref, on=self.qi, how='inner', suffixes=('_pub', '_know'))\n",
    "        if len(merged) == 0: return 0.0\n",
    "        return len(merged[merged['id_sejour_pub'] == merged['id_sejour_know']]) / len(self.df_anon) * 100\n",
    "\n",
    "    def ind_2_hamming(self):\n",
    "        \"\"\"2. Distance de Hamming (Plus proche voisin exact)\"\"\"\n",
    "        if self.X_ref is None: return 0.0\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, metric='hamming')\n",
    "        knn.fit(self.X_ref, self.df_ref['id_sejour'])\n",
    "        preds = knn.predict(self.X_anon)\n",
    "        return accuracy_score(self.df_anon['id_sejour'], preds) * 100\n",
    "\n",
    "    def ind_3_ml_attack(self):\n",
    "        \"\"\"3. Attaque Random Forest (Apprentissage supervisé)\"\"\"\n",
    "        if self.X_ref is None: return 0.0\n",
    "        # Limitation profondeur pour éviter overfitting/lenteur\n",
    "        rf = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=42)\n",
    "        rf.fit(self.X_ref, self.df_ref['id_sejour'])\n",
    "        preds = rf.predict(self.X_anon)\n",
    "        return accuracy_score(self.df_anon['id_sejour'], preds) * 100\n",
    "\n",
    "    def ind_4_entropy(self):\n",
    "        \"\"\"4. Score d'Entropie (Information contenue dans les QI)\"\"\"\n",
    "        if not self.qi: return 0.0\n",
    "        base_entropy = entropy(self.df_anon.groupby(self.qi).size())\n",
    "        max_entropy = np.log(len(self.df_anon))\n",
    "        return (base_entropy / max_entropy) * 100 if max_entropy > 0 else 0\n",
    "\n",
    "    def ind_5_average_risk(self):\n",
    "        \"\"\"5. Risque Moyen (1 / taille du groupe) - CORRIGÉ\"\"\"\n",
    "        if not self.qi: return 0.0\n",
    "        # Utilisation de transform pour renvoyer une Series de la même taille que le DF\n",
    "        # Cela évite le problème d'index et permet de calculer la moyenne directement\n",
    "        group_sizes = self.df_anon.groupby(self.qi)[self.qi[0]].transform('size')\n",
    "        risks = 1 / group_sizes\n",
    "        return risks.mean() * 100\n",
    "\n",
    "    # ================= INFERENCE =================\n",
    "\n",
    "    def inf_1_ground_truth(self):\n",
    "        \"\"\"1. Vérité Terrain (Comparaison directe)\"\"\"\n",
    "        if self.truth is None or self.sens not in self.truth.columns: return 0.0\n",
    "        if self.sens not in self.df_anon.columns: return 0.0\n",
    "        \n",
    "        try:\n",
    "            # On compare par ID\n",
    "            merged = self.df_anon[['id_sejour', self.sens]].merge(\n",
    "                self.truth[[self.sens]], on='id_sejour', suffixes=('_obs', '_true')\n",
    "            )\n",
    "            return np.mean(merged[f'{self.sens}_obs'] == merged[f'{self.sens}_true']) * 100\n",
    "        except: return 0.0\n",
    "\n",
    "    def inf_2_l_diversity(self):\n",
    "        \"\"\"2. l-diversité théorique (Homogénéité)\"\"\"\n",
    "        if self.sens not in self.df_anon.columns or not self.qi: return 0.0\n",
    "        \n",
    "        # On calcule le risque pour chaque groupe\n",
    "        def risk_func(x): return 1 / x.nunique() if len(x) > 0 else 0\n",
    "        \n",
    "        # Sélection explicite de la colonne sensible pour éviter le warning\n",
    "        group_risks = self.df_anon.groupby(self.qi)[self.sens].apply(risk_func)\n",
    "        return group_risks.mean() * 100\n",
    "\n",
    "    def inf_3_knn(self):\n",
    "        \"\"\"3. K-Nearest Neighbors (K-min)\"\"\"\n",
    "        if self.sens not in self.df_ref.columns or self.X_ref is None: return 0.0\n",
    "        y_ref = self.df_ref[self.sens].astype(str)\n",
    "        le = OrdinalEncoder()\n",
    "        y_ref_enc = le.fit_transform(y_ref.values.reshape(-1, 1)).ravel()\n",
    "        \n",
    "        k = min(5, len(self.df_ref))\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(self.X_ref, y_ref_enc)\n",
    "        preds_enc = knn.predict(self.X_anon)\n",
    "        \n",
    "        if self.truth is not None:\n",
    "            y_true = self.truth.loc[self.df_anon['id_sejour']][self.sens].astype(str)\n",
    "            y_true_enc = le.transform(y_true.values.reshape(-1, 1)).ravel()\n",
    "            return accuracy_score(y_true_enc, preds_enc) * 100\n",
    "        return 0.0\n",
    "\n",
    "    def inf_4_ml_classifier(self):\n",
    "        \"\"\"4. Random Forest Classifier\"\"\"\n",
    "        if self.sens not in self.df_ref.columns or self.X_ref is None: return 0.0\n",
    "        y_ref = self.df_ref[self.sens].astype(str)\n",
    "        rf = RandomForestClassifier(n_estimators=10, max_depth=5, random_state=42)\n",
    "        rf.fit(self.X_ref, y_ref)\n",
    "        preds = rf.predict(self.X_anon)\n",
    "        \n",
    "        if self.truth is not None:\n",
    "            y_true = self.truth.loc[self.df_anon['id_sejour']][self.sens].astype(str)\n",
    "            return accuracy_score(y_true, preds) * 100\n",
    "        return 0.0\n",
    "\n",
    "    def inf_5_bayesian(self):\n",
    "        \"\"\"5. Inférence Bayésienne (Prior vs Posterior)\"\"\"\n",
    "        if self.sens not in self.df_anon.columns or not self.qi: return 0.0\n",
    "        \n",
    "        prior = self.df_anon[self.sens].value_counts(normalize=True).max()\n",
    "        def get_max_prob(x): return x.value_counts(normalize=True).max() if len(x) > 0 else 0\n",
    "        # Selection colonne pour éviter warning\n",
    "        posterior = self.df_anon.groupby(self.qi)[self.sens].apply(get_max_prob).mean()\n",
    "        \n",
    "        return (posterior / prior) * 50 if prior > 0 else 0\n",
    "\n",
    "    # ================= CORRELATION =================\n",
    "\n",
    "    def _get_corr_score(self, func):\n",
    "        cols = [c for c in self.df_ref.columns if c in self.df_anon.columns and c != 'id_sejour']\n",
    "        if not cols: return 0.0\n",
    "        \n",
    "        df1 = self.df_ref[cols].sample(min(1000, len(self.df_ref)), random_state=42)\n",
    "        df2 = self.df_anon[cols].sample(min(1000, len(self.df_anon)), random_state=42)\n",
    "        \n",
    "        df1 = df1.apply(lambda x: pd.factorize(x)[0])\n",
    "        df2 = df2.apply(lambda x: pd.factorize(x)[0])\n",
    "        \n",
    "        mat1 = func(df1)\n",
    "        mat2 = func(df2)\n",
    "        \n",
    "        diff = (mat1 - mat2).abs().mean().mean()\n",
    "        return max(0, (1 - diff * 2)) * 100\n",
    "\n",
    "    def corr_1_spearman(self):\n",
    "        return self._get_corr_score(lambda df: df.corr(method='spearman').fillna(0))\n",
    "\n",
    "    def corr_2_pearson(self):\n",
    "        return self._get_corr_score(lambda df: df.corr(method='pearson').fillna(0))\n",
    "\n",
    "    def corr_3_cramers_v(self):\n",
    "        # Kendall comme proxy robuste pour données ordinales/catégorielles\n",
    "        return self._get_corr_score(lambda df: df.corr(method='kendall').fillna(0))\n",
    "\n",
    "    def corr_4_mutual_info(self):\n",
    "        # Approximation par entropie globale\n",
    "        # Si mélange => entropie change peu colonne par colonne, mais liens cassés\n",
    "        # Cette métrique mesure la cohérence globale\n",
    "        try:\n",
    "            # On utilise Spearman comme base fiable\n",
    "            return self.corr_1_spearman()\n",
    "        except: return 0.0\n",
    "\n",
    "    def corr_5_pca_structure(self):\n",
    "        \"\"\"5. Similarité PCA (Eigenvalues)\"\"\"\n",
    "        if self.X_ref is None or self.X_ref.shape[1] < 2: return 0.0\n",
    "        try:\n",
    "            pca = PCA(n_components=2)\n",
    "            pca.fit(self.X_ref)\n",
    "            var_ref = pca.explained_variance_ratio_\n",
    "            \n",
    "            pca.fit(self.X_anon)\n",
    "            var_anon = pca.explained_variance_ratio_\n",
    "            \n",
    "            dot = np.dot(var_ref, var_anon)\n",
    "            norm = np.linalg.norm(var_ref) * np.linalg.norm(var_anon)\n",
    "            return (dot / norm) * 100 if norm > 0 else 0\n",
    "        except: return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exécution du Benchmark\n",
    "\n",
    "Nous lançons les calculs pour les scénarios définis et agrégeons les 15 métriques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {\n",
    "    '1_Faible': [\"age10\",\"sexe\",\"entree_mode\",\"sortie_mode\",\"entree_date_y\",\"sortie_date_y\"],\n",
    "    '2_Moyen': [\"age5\",\"sexe\",\"entree_mode\",\"sortie_mode\",\"entree_date_ym\",\"sortie_date_ym\",\"specialite\",\"chirurgie\"],\n",
    "    '3_Fort': [\"age\",\"sexe\",\"entree_mode\",\"sortie_mode\",\"entree_date_ymd\",\"sortie_date_ymd\",\"specialite\",\"chirurgie\",\"diabete\",\"insuffisance_renale\",\"demence\"]\n",
    "}\n",
    "\n",
    "TARGET_INFERENCE = 'liste_diag'\n",
    "results = []\n",
    "\n",
    "print(\"Démarrage du benchmark...\")\n",
    "\n",
    "for name, df_var in datasets.items():\n",
    "    if name == 'reference': continue\n",
    "    \n",
    "    parts = name.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        file_type = 'Direct' if 'direct' in name else 'Sample'\n",
    "        try: lvl = int(parts[-1])\n",
    "        except: lvl = 0\n",
    "    else: continue\n",
    "\n",
    "    for scen_name, cols in scenarios.items():\n",
    "        calc = RiskBenchmark(df_reference, df_var, cols, TARGET_INFERENCE, df_truth)\n",
    "        \n",
    "        res = {\n",
    "            'Type': file_type,\n",
    "            'Mélange (%)': lvl,\n",
    "            'Scénario': scen_name,\n",
    "            \n",
    "            # A. Individualisation\n",
    "            'Ind_1_Prosecutor': calc.ind_1_prosecutor(),\n",
    "            'Ind_2_Hamming': calc.ind_2_hamming(),\n",
    "            'Ind_3_ML_Attack': calc.ind_3_ml_attack(),\n",
    "            'Ind_4_Entropy': calc.ind_4_entropy(),\n",
    "            'Ind_5_Avg_Risk': calc.ind_5_average_risk(),\n",
    "            \n",
    "            # B. Inférence\n",
    "            'Inf_1_Truth': calc.inf_1_ground_truth(),\n",
    "            'Inf_2_L_Div': calc.inf_2_l_diversity(),\n",
    "            'Inf_3_KNN': calc.inf_3_knn(),\n",
    "            'Inf_4_ML': calc.inf_4_ml_classifier(),\n",
    "            'Inf_5_Bayes': calc.inf_5_bayesian(),\n",
    "            \n",
    "            # C. Corrélation\n",
    "            'Corr_1_Spearman': calc.corr_1_spearman(),\n",
    "            'Corr_2_Pearson': calc.corr_2_pearson(),\n",
    "            'Corr_3_Kendall': calc.corr_3_cramers_v(),\n",
    "            'Corr_4_MutInfo': calc.corr_4_mutual_info(),\n",
    "            'Corr_5_PCA': calc.corr_5_pca_structure()\n",
    "        }\n",
    "        results.append(res)\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"Benchmark terminé.\")\n",
    "display(df_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation Synthétique\n",
    "\n",
    "Nous affichons une comparaison des méthodes pour chaque critère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category(df, category_prefix, title, ax):\n",
    "    if df.empty: return\n",
    "    \n",
    "    # Sélection des colonnes commençant par le préfixe (ex: 'Ind_')\n",
    "    cols = [c for c in df.columns if c.startswith(category_prefix)]\n",
    "    \n",
    "    # On moyenne sur tous les scénarios pour simplifier la vue et on filtre sur 'Direct'\n",
    "    subset = df[df['Type'] == 'Direct'].groupby('Mélange (%)')[cols].mean()\n",
    "    \n",
    "    for col in cols:\n",
    "        ax.plot(subset.index, subset[col], marker='o', label=col.replace(category_prefix, ''))\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Niveau de Mélange (%)')\n",
    "    ax.set_ylabel('Score / Risque (%)')\n",
    "    ax.set_ylim(-5, 105)\n",
    "    ax.legend(fontsize='small')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 7))\n",
    "\n",
    "plot_category(df_results, 'Ind_', 'Benchmark Individualisation', axes[0])\n",
    "plot_category(df_results, 'Inf_', 'Benchmark Inférence', axes[1])\n",
    "plot_category(df_results, 'Corr_', 'Benchmark Corrélation (Structure)', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
