{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Évaluation du risque de réidentification dans les bases de données de santé\n",
    "## Projet M2 MIAS - Février 2026\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Contexte et Objectifs\n",
    "Ce projet vise à imaginer et tester des indicateurs quantitatifs du risque de réidentification d'individus dans une base de données, conformément aux critères du G29 (CNIL) : **Inférence**, **Corrélation**, et **Individualisation**.\n",
    "\n",
    "Nous travaillerons sur des données synthétiques simulant des séjours hospitaliers et analyserons l'impact de deux techniques de protection :\n",
    "1. **Mélange de valeurs (Permutation)** : Échange de valeurs entre individus pour une même variable.\n",
    "2. **Échantillonnage avec remise** : Création d'une base issue d'un tirage aléatoire, modifiant la population initiale.\n",
    "\n",
    "Les fichiers analysés sont :\n",
    "- `connaissances_externes.txt` : La base de référence (population source).\n",
    "- `out_direct_XX.txt` : Base avec XX% de mélange (conservation de la population).\n",
    "- `out_sample_XX.txt` : Base échantillonnée avec XX% de mélange."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration et Simulation des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration graphique\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(2026)\n",
    "\n",
    "def simuler_base_donnees(n=1000):\n",
    "    \"\"\"Génère une base de données synthétique réaliste\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    df['id_sejour'] = range(1, n+1)\n",
    "    \n",
    "    # Données démographiques\n",
    "    ages = np.random.randint(18, 95, n)\n",
    "    df['age'] = ages\n",
    "    df['age5'] = (ages // 5) * 5\n",
    "    df['age10'] = (ages // 10) * 10\n",
    "    df['sexe'] = np.random.choice(['M', 'F'], n, p=[0.48, 0.52])\n",
    "    \n",
    "    # Données temporelles\n",
    "    dates_entree = pd.date_range('2023-01-01', periods=n, freq='H')\n",
    "    durees = np.random.exponential(5, n).astype(int) + 1\n",
    "    dates_sortie = dates_entree + pd.to_timedelta(durees, unit='D')\n",
    "    \n",
    "    df['entree_date_ymd'] = dates_entree.strftime('%Y-%m-%d')\n",
    "    df['entree_date_ym'] = dates_entree.strftime('%Y-%m')\n",
    "    df['entree_date_y'] = dates_entree.strftime('%Y')\n",
    "    \n",
    "    # Données médicales\n",
    "    specialites = ['Cardio', 'Pneumo', 'Gastro', 'Neuro', 'Trauma', 'Onco']\n",
    "    df['specialite'] = np.random.choice(specialites, n)\n",
    "    df['chirurgie'] = np.random.choice([0, 1], n, p=[0.7, 0.3])\n",
    "    df['diabete'] = np.random.choice([0, 1], n, p=[0.85, 0.15])\n",
    "    \n",
    "    # Mélange initial des colonnes\n",
    "    cols = list(df.columns)\n",
    "    cols.remove('id_sejour')\n",
    "    np.random.shuffle(cols)\n",
    "    return df[['id_sejour'] + cols]\n",
    "\n",
    "# Création de la base de référence\n",
    "base_reference = simuler_base_donnees(1000)\n",
    "print(f\"Base générée : {base_reference.shape}\")\n",
    "base_reference.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Génération des fichiers modifiés (Direct et Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melanger_colonnes(df, pourcentage):\n",
    "    \"\"\"Mélange aléatoire des valeurs dans chaque colonne\"\"\"\n",
    "    df_res = df.copy()\n",
    "    if pourcentage == 0: return df_res\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == 'id_sejour': continue\n",
    "        n_mix = int(len(df) * pourcentage / 100)\n",
    "        idx = np.random.choice(len(df), n_mix, replace=False)\n",
    "        vals = df_res.loc[idx, col].values\n",
    "        np.random.shuffle(vals)\n",
    "        df_res.loc[idx, col] = vals\n",
    "    return df_res\n",
    "\n",
    "def echantillonner(df, pourcentage_melange):\n",
    "    \"\"\"Tirage avec remise puis mélange\"\"\"\n",
    "    idx = np.random.choice(len(df), len(df), replace=True)\n",
    "    df_sample = df.iloc[idx].reset_index(drop=True)\n",
    "    # Conservation des IDs pour vérification (théorique)\n",
    "    df_sample['id_sejour'] = df['id_sejour'].iloc[idx].values\n",
    "    return melanger_colonnes(df_sample, pourcentage_melange)\n",
    "\n",
    "fichiers = {'reference': base_reference}\n",
    "niveaux = [0, 10, 20, 30, 40, 50]\n",
    "\n",
    "for n in niveaux:\n",
    "    fichiers[f'direct_{n}'] = melanger_colonnes(base_reference, n)\n",
    "    fichiers[f'sample_{n}'] = echantillonner(base_reference, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Définition des Indicateurs de Risque\n",
    "\n",
    "Nous implémentons trois métriques principales :\n",
    "1. **Taux d'individualisation** : Pourcentage d'enregistrements uniques dans la base anonymisée qui correspondent à un individu unique dans la base source (réidentification réussie).\n",
    "2. **Risque d'inférence** : Capacité à deviner une valeur sensible (ex: diabète) connaissant les autres attributs.\n",
    "3. **Risque de corrélation (Entropie)** : Mesure de la perte d'information/structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indicateurs:\n",
    "    @staticmethod\n",
    "    def individualisation(df_base, df_anon, qi_vars):\n",
    "        \"\"\"Calcule le taux de réidentification correcte unique\"\"\"\n",
    "        # Dans la base anon, combien sont uniques sur les quasi-identifiants ?\n",
    "        groupes = df_anon.groupby(qi_vars).size()\n",
    "        uniques_anon = groupes[groupes == 1].index\n",
    "        \n",
    "        # Filtrer la base anon pour ne garder que ces uniques\n",
    "        df_unique = df_anon.set_index(qi_vars).loc[uniques_anon].reset_index()\n",
    "        \n",
    "        reussites = 0\n",
    "        # Vérification avec la base source (attaque par jointure)\n",
    "        # Note: Optimisé pour la démonstration\n",
    "        for _, row in df_unique.iterrows():\n",
    "            # Trouver les correspondances dans la source\n",
    "            mask = np.ones(len(df_base), dtype=bool)\n",
    "            for v in qi_vars:\n",
    "                mask &= (df_base[v] == row[v])\n",
    "            match = df_base[mask]\n",
    "            \n",
    "            if len(match) == 1:\n",
    "                # Succès si l'ID correspond (l'attaquant ne connait pas l'ID, mais nous vérifions le risque)\n",
    "                if match.iloc[0]['id_sejour'] == row['id_sejour']:\n",
    "                    reussites += 1\n",
    "        \n",
    "        return (reussites / len(df_anon)) * 100\n",
    "\n",
    "    @staticmethod\n",
    "    def inference(df_base, df_anon, target_var):\n",
    "        \"\"\"Mesure la précision de l'inférence simple (valeur conservée)\"\"\"\n",
    "        # Si les données sont mélangées, la valeur observée peut être fausse\n",
    "        # On mesure ici simplement si la valeur dans anon correspond à la valeur réelle\n",
    "        # pour les mêmes lignes (simplification)\n",
    "        if len(df_base) != len(df_anon): return 0 # Non applicable directement sur sample sans alignement\n",
    "        correct = (df_base[target_var].values == df_anon[target_var].values).sum()\n",
    "        return correct / len(df_base) * 100\n",
    "\n",
    "    @staticmethod\n",
    "    def unicite(df, vars):\n",
    "        return (df.groupby(vars).size() == 1).sum() / len(df) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse par Scénarios\n",
    "\n",
    "Définition de scénarios de connaissances de l'attaquant, allant de faible à fort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = [\n",
    "    {'nom': 'S1: Faible', 'vars': ['age10', 'sexe', 'entree_date_y']},\n",
    "    {'nom': 'S2: Moyen', 'vars': ['age5', 'sexe', 'entree_date_ym', 'specialite']},\n",
    "    {'nom': 'S3: Fort', 'vars': ['age', 'sexe', 'entree_date_ymd', 'specialite', 'chirurgie']}\n",
    "]\n",
    "\n",
    "resultats = []\n",
    "\n",
    "for nom_f, df in fichiers.items():\n",
    "    if nom_f == 'reference': continue\n",
    "    type_f = 'Direct' if 'direct' in nom_f else 'Sample'\n",
    "    niveau = int(nom_f.split('_')[1])\n",
    "    \n",
    "    for scen in scenarios:\n",
    "        risk_ind = Indicateurs.individualisation(base_reference, df, scen['vars'])\n",
    "        unicite_val = Indicateurs.unicite(df, scen['vars'])\n",
    "        \n",
    "        res = {\n",
    "            'Fichier': type_f,\n",
    "            'Mélange (%)': niveau,\n",
    "            'Scénario': scen['nom'],\n",
    "            'Risque Individualisation (%)': risk_ind,\n",
    "            'Unicité (%)': unicite_val\n",
    "        }\n",
    "        resultats.append(res)\n",
    "\n",
    "df_res = pd.DataFrame(resultats)\n",
    "df_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation des Résultats\n",
    "\n",
    "### 6.1 Impact du Mélange sur le Risque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=df_res[df_res['Fichier'] == 'Direct'], \n",
    "             x='Mélange (%)', y='Risque Individualisation (%)', \n",
    "             hue='Scénario', marker='o')\n",
    "plt.title(\"Évolution du Risque d'Individualisation (Fichiers Directs)\")\n",
    "plt.ylim(0, 105)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Comparaison Direct vs Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=df_res, x='Mélange (%)', y='Risque Individualisation (%)', \n",
    "             hue='Fichier', style='Scénario', markers=True, dashes=False)\n",
    "plt.title(\"Comparaison de l'efficacité : Permutation (Direct) vs Échantillonnage (Sample)\")\n",
    "plt.ylabel(\"Taux de Réidentification réussie (%)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Discussion et Conclusion\n",
    "\n",
    "### Analyse des résultats\n",
    "\n",
    "1.  **Impact des connaissances (Scénarios) :**\n",
    "    * Plus l'attaquant possède d'informations précises (Scénario 3 vs Scénario 1), plus le risque de réidentification est élevé (proche de 100% sans protection). L'unicité des individus augmente drastiquement avec le nombre de variables.\n",
    "\n",
    "2.  **Effet du Mélange (Permutation) :**\n",
    "    * Le risque diminue linéairement avec le taux de mélange. En cassant les corrélations entre les quasi-identifiants (ex: âge, code postal) et les identifiants sensibles, on réduit la capacité à isoler un individu unique avec certitude.\n",
    "\n",
    "3.  **Supériorité de l'Échantillonnage (`Sample` vs `Direct`) :**\n",
    "    * Comme le montre le graphique comparatif, la méthode **Sample (Échantillonnage avec remise)** offre systématiquement une meilleure protection (risque plus faible) à niveau de mélange égal.\n",
    "    * **Pourquoi ?** L'échantillonnage introduit une incertitude fondamentale : un individu présent dans la base anonymisée n'est pas forcément unique dans la population (duplication), et un individu de la population cible peut ne pas être dans la base (absence).\n",
    "    * Dans le fichier `Direct`, la population est conservée (bijection) : si je trouve un profil unique qui correspond à ma cible, c'est forcément elle (sauf si les données sont mélangées). Dans le fichier `Sample`, même une correspondance unique n'est pas une preuve formelle d'appartenance.\n",
    "\n",
    "### Conclusion\n",
    "Pour minimiser le risque de réidentification, il est recommandé de combiner la **généralisation** des données (réduire la précision, cf. Scénario 1) et l'**échantillonnage**, plutôt que de simplement mélanger les données d'une population exhaustive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
